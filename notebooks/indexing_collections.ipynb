{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00a9e88-1fcb-4af5-9001-cb10c3fc184d",
   "metadata": {},
   "source": [
    "# Qdrant Collection Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60502993-d41b-4532-8c1b-a3be0af8a70b",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "544405c0-e893-4d59-9d5c-7e72c1da90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    OptimizersConfig,\n",
    "    ScalarQuantizationConfig,\n",
    "    ScalarType,\n",
    "    ScalarQuantization,\n",
    "    OptimizersConfigDiff,\n",
    "    HnswConfigDiff,\n",
    "    PointStruct,\n",
    "    Filter,\n",
    "    FieldCondition,\n",
    "    MatchValue,\n",
    "    Nested,\n",
    "    NestedCondition,\n",
    "    MatchAny\n",
    ")\n",
    "from sklearn.datasets import make_blobs\n",
    "from ray.util.multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7a02b-29dd-4abc-90d8-9f070b9498af",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fba11c2-6752-4405-b770-a094c83c5082",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HOST = 'localhost'\n",
    "QDRANT_PORT = 6333\n",
    "QDRANT_GRPC_PORT = 6334\n",
    "\n",
    "CLIENT = QdrantClient(\n",
    "    url=QDRANT_HOST, \n",
    "    port=QDRANT_PORT, \n",
    "    # grpc_port=QDRANT_GRPC_PORT,\n",
    "    # prefer_grpc=True\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = 'test_collection'\n",
    "\n",
    "NUMBER_OF_FEATURES = 192\n",
    "NUMBER_OF_POINTS = 150_000\n",
    "NUMBER_OF_CENTERS = 25\n",
    "\n",
    "CATEGORIES = ['c1', 'c2', 'c3', 'c4', 'c5', 'c6', ]\n",
    "ASSORTMENT = [\n",
    "    ['a1', 'a3'],\n",
    "    ['a3'],\n",
    "    ['a1', 'a2'],\n",
    "    ['a4'],\n",
    "    ['a2', 'a7'],\n",
    "    ['a2'],\n",
    "    ['a4', 'a2'],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40b7e91-91fb-40e6-8424-10b9052a87ac",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c5a356-ee11-41c0-83e1-05f56cc6ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_info():\n",
    "    collection_info = CLIENT.get_collection(COLLECTION_NAME)\n",
    "    info = f\"\"\"\n",
    "name: {COLLECTION_NAME} status: {collection_info.status.value}\n",
    "=======================================================================\n",
    "points count: {collection_info.points_count}\n",
    "vector count: {collection_info.vectors_count}\n",
    "indexed vectors count: {collection_info.indexed_vectors_count}\n",
    "\n",
    "collection info: {collection_info}\n",
    "\"\"\"\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e38ec2d-0d3b-440c-b0f3-6cf390c90eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors(n: int = NUMBER_OF_POINTS) -> list:\n",
    "    vectors, _ = make_blobs(n_samples=n,\n",
    "                            n_features=NUMBER_OF_FEATURES,\n",
    "                            centers=NUMBER_OF_CENTERS,\n",
    "                            cluster_std=2.0,\n",
    "                            center_box=(-5, 5),\n",
    "                            shuffle=True,\n",
    "                            random_state=1)\n",
    "    return [v.tolist() for v in vectors]\n",
    "\n",
    "\n",
    "def create_points(n: int = NUMBER_OF_POINTS) -> list:\n",
    "    vectors = create_vectors(n)\n",
    "    points = []\n",
    "    for i, vector in enumerate(vectors):\n",
    "        point = {\n",
    "            'id': i+1,\n",
    "            'vector': vector,\n",
    "            'payload': {\n",
    "                'category': CATEGORIES[random.randint(0, len(CATEGORIES)-1)],\n",
    "                'stores': [\n",
    "                    {'id': j, 'assortment': ASSORTMENT[random.randint(0, len(ASSORTMENT)-1)]}\n",
    "                for j in range(0, 25)],\n",
    "            }\n",
    "        }\n",
    "        points.append(point)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7e56a9-14be-4ca4-8d73-0f371a468916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_points(points: list):\n",
    "    client = QdrantClient(host=QDRANT_HOST,\n",
    "                          port=QDRANT_PORT,\n",
    "                          grpc_port=QDRANT_GRPC_PORT,\n",
    "                          prefer_grpc=True,\n",
    "                          timeout=100_000)\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME, \n",
    "        points=[PointStruct(id=p['id'], \n",
    "                        vector=p['vector'], \n",
    "                        payload=p['payload']) for p in points]\n",
    "    )\n",
    "\n",
    "\n",
    "def chunks(l: list, n: int):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e877643-8b2d-41dd-bd0b-de0a32551ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_PARAMS = VectorParams(size=NUMBER_OF_FEATURES, distance=Distance.COSINE)\n",
    "QUANTIZATION_CONFIG = ScalarQuantization(scalar=ScalarQuantizationConfig(type=ScalarType.INT8, always_ram=True))\n",
    "OPTIMIZERS_CONFIG = OptimizersConfigDiff(memmap_threshold=20_000, indexing_threshold=1)\n",
    "HNSW_CONFIG = HnswConfigDiff(m=16, ef_construct=100, full_scan_threshold=10_000)\n",
    "\n",
    "\n",
    "def create_index():\n",
    "    field = 'category'\n",
    "    CLIENT.create_payload_index(COLLECTION_NAME, field, 'keyword')\n",
    "    # while CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points != NUMBER_OF_POINTS:\n",
    "    #     print(f\"Zzzz... Waiting for index on {field} to be created. ({CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points}/{NUMBER_OF_POINTS}).\")\n",
    "    #     time.sleep(5)\n",
    "\n",
    "    field = 'stores[].id'\n",
    "    CLIENT.create_payload_index(COLLECTION_NAME, field, 'integer')\n",
    "    # while CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points != NUMBER_OF_POINTS:\n",
    "    #     print(f\"Zzzz... Waiting for index on {field} to be created. ({CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points}/{NUMBER_OF_POINTS}).\")\n",
    "    #     time.sleep(5)\n",
    "\n",
    "    field = 'stores[].assortment'\n",
    "    CLIENT.create_payload_index(COLLECTION_NAME, field, 'keyword')\n",
    "    # while CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points != NUMBER_OF_POINTS:\n",
    "    #     print(f\"Zzzz... Waiting for index on {field} to be created. ({CLIENT.get_collection(COLLECTION_NAME).payload_schema[field].points}/{NUMBER_OF_POINTS}).\")\n",
    "    #     time.sleep(5)\n",
    "\n",
    "                             \n",
    "def create_collection(points: list):\n",
    "    CLIENT.recreate_collection(collection_name=COLLECTION_NAME,\n",
    "                               vectors_config=VECTOR_PARAMS,\n",
    "                               shard_number=1,\n",
    "                               on_disk_payload=False,\n",
    "                               optimizers_config=OPTIMIZERS_CONFIG,\n",
    "                               quantization_config=QUANTIZATION_CONFIG)\n",
    "    create_index()\n",
    "    \n",
    "    pool = Pool()\n",
    "    pool.map(upsert_points, chunks(points, 2_000))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # for chunk in chunks(points, 2_000):\n",
    "    #     upsert_points(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97308a6-5a22-41b0-b68c-b967b33728e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_collection(query_vector: list, filter=None, n=10) -> float:\n",
    "    durations = []\n",
    "    for _ in range(0, n):\n",
    "        start_time = time.time()\n",
    "        hits = CLIENT.search(collection_name=COLLECTION_NAME,\n",
    "                             query_vector=query_vector,\n",
    "                             query_filter=filter,\n",
    "                             limit=10,\n",
    "                             with_payload=False,\n",
    "                             with_vectors=False)\n",
    "        durations.append(time.time() - start_time)\n",
    "    return sum(durations) / len(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae58ba3-6cf6-4705-a954-5eee4a164d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_level_filter() -> Filter:\n",
    "    return Filter(must=[FieldCondition(key='category', match=MatchValue(value='c3'))])\n",
    "\n",
    "\n",
    "def get_nested_filter() -> Filter:\n",
    "    return Filter(\n",
    "        must=[\n",
    "            NestedCondition(\n",
    "                nested=Nested(\n",
    "                    key='stores', \n",
    "                    filter=Filter(\n",
    "                        must=[\n",
    "                            FieldCondition(key='id', match=MatchValue(value=4)),\n",
    "                            FieldCondition(key='assortment', match=MatchAny(any=['a1'])),\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb31cb-459b-4866-8c37-1d7dfc89576e",
   "metadata": {},
   "source": [
    "## Create Collection & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bfb1fd5-54dc-41f8-bafb-f0fc49434efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 13:05:31,026\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "create_collection(create_points(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e213ef7c-7a21-46df-b1ed-80f6590090fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while CLIENT.get_collection(COLLECTION_NAME).status.value == 'yellow':\n",
    "    time.sleep(10)\n",
    "    get_collection_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b820ca1-7849-43b0-a7ae-2f50ad2bf178",
   "metadata": {},
   "source": [
    "## Search Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dc7824d-4c02-48c7-acfd-8c6b237dc03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[query_vector] = create_vectors(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcaece-448c-4094-8502-bf3cad24af26",
   "metadata": {},
   "source": [
    "### W/O Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66ddbd4-9550-4640-9499-097aa683c545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005563859939575195"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_collection(query_vector, n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e4989-e349-40b0-b603-5949a6688538",
   "metadata": {},
   "source": [
    "### Top Level Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28e03cb3-f31b-44ce-9281-36ad5bbd536c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025892186164855955"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_collection(query_vector, filter=get_top_level_filter(), n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ee77b-88ad-43e9-96cb-360a928f3ebd",
   "metadata": {},
   "source": [
    "### Nested Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d8d9b69-c519-4651-875f-99b201a38e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035550570487976074"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_collection(query_vector, filter=get_nested_filter(), n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17e4e6-aa72-4d83-8f41-74490fd8cd1c",
   "metadata": {},
   "source": [
    "# Big Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55769f86-0f8f-43a2-a63d-8de34026d648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection with 1 points.\n",
      "Waiting for collection status to be green.\n",
      "Creating collection with 10000 points.\n",
      "Waiting for collection status to be green.\n",
      "Creating collection with 20000 points.\n",
      "Waiting for collection status to be green.\n",
      "Creating collection with 30000 points.\n",
      "Waiting for collection status to be green.\n",
      "Creating collection with 40000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 40000\n",
      "vector count: 40000\n",
      "indexed vectors count: 40000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=40000 indexed_vectors_count=40000 points_count=40000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=40000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=40000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=40000)}\n",
      "\n",
      "Creating collection with 50000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 50000\n",
      "vector count: 50000\n",
      "indexed vectors count: 50000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=50000 indexed_vectors_count=50000 points_count=50000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=50000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=50000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=50000)}\n",
      "\n",
      "Creating collection with 60000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 60000\n",
      "vector count: 60000\n",
      "indexed vectors count: 50000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=60000 indexed_vectors_count=50000 points_count=60000 segments_count=3 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=60000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=60000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=60000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 60000\n",
      "vector count: 60000\n",
      "indexed vectors count: 60000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=60000 indexed_vectors_count=60000 points_count=60000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=60000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=60000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=60000)}\n",
      "\n",
      "Creating collection with 70000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 70000\n",
      "vector count: 70000\n",
      "indexed vectors count: 32000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=70000 indexed_vectors_count=32000 points_count=70000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=70000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=70000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=70000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 70000\n",
      "vector count: 70000\n",
      "indexed vectors count: 70000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=70000 indexed_vectors_count=70000 points_count=70000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=70000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=70000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=70000)}\n",
      "\n",
      "Creating collection with 80000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 80000\n",
      "vector count: 80000\n",
      "indexed vectors count: 32000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=80000 indexed_vectors_count=32000 points_count=80000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=80000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000)}\n",
      "\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 80000\n",
      "vector count: 80000\n",
      "indexed vectors count: 64000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=80000 indexed_vectors_count=64000 points_count=80000 segments_count=3 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=80000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 80000\n",
      "vector count: 80000\n",
      "indexed vectors count: 80000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=80000 indexed_vectors_count=80000 points_count=80000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=80000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=80000)}\n",
      "\n",
      "Creating collection with 90000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 90000\n",
      "vector count: 90000\n",
      "indexed vectors count: 0\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=90000 indexed_vectors_count=0 points_count=90000 segments_count=5 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=90000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000)}\n",
      "\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 90000\n",
      "vector count: 90000\n",
      "indexed vectors count: 38000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=90000 indexed_vectors_count=38000 points_count=90000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=90000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 90000\n",
      "vector count: 90000\n",
      "indexed vectors count: 90000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=90000 indexed_vectors_count=90000 points_count=90000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=90000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=90000)}\n",
      "\n",
      "Creating collection with 100000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 100000\n",
      "vector count: 100000\n",
      "indexed vectors count: 0\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=100000 indexed_vectors_count=0 points_count=100000 segments_count=5 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=100000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000)}\n",
      "\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 100000\n",
      "vector count: 100000\n",
      "indexed vectors count: 40000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=100000 indexed_vectors_count=40000 points_count=100000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=100000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 100000\n",
      "vector count: 100000\n",
      "indexed vectors count: 100000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=100000 indexed_vectors_count=100000 points_count=100000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=100000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=100000)}\n",
      "\n",
      "Creating collection with 110000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 110000\n",
      "vector count: 110000\n",
      "indexed vectors count: 0\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=110000 indexed_vectors_count=0 points_count=110000 segments_count=5 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=110000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000)}\n",
      "\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 110000\n",
      "vector count: 110000\n",
      "indexed vectors count: 44000\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=110000 indexed_vectors_count=44000 points_count=110000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=110000), 'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000)}\n",
      "\n",
      "\n",
      "name: test_collection status: green\n",
      "=======================================================================\n",
      "points count: 110000\n",
      "vector count: 110000\n",
      "indexed vectors count: 110000\n",
      "\n",
      "collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=110000 indexed_vectors_count=110000 points_count=110000 segments_count=4 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=110000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=110000)}\n",
      "\n",
      "Creating collection with 120000 points.\n",
      "Waiting for collection status to be green.\n",
      "\n",
      "name: test_collection status: yellow\n",
      "=======================================================================\n",
      "points count: 120000\n",
      "vector count: 120000\n",
      "indexed vectors count: 0\n",
      "\n",
      "collection info: status=<CollectionStatus.YELLOW: 'yellow'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=120000 indexed_vectors_count=0 points_count=120000 segments_count=5 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=192, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, replication_factor=1, write_consistency_factor=1, on_disk_payload=False), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=20000, indexing_threshold=1, flush_interval_sec=5, max_optimization_threads=1), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=ScalarQuantization(scalar=ScalarQuantizationConfig(type=<ScalarType.INT8: 'int8'>, quantile=None, always_ram=True))) payload_schema={'category': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=120000), 'stores[].assortment': PayloadIndexInfo(data_type=<PayloadSchemaType.KEYWORD: 'keyword'>, params=None, points=120000), 'stores[].id': PayloadIndexInfo(data_type=<PayloadSchemaType.INTEGER: 'integer'>, params=None, points=120000)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for number_of_points in range(0, NUMBER_OF_POINTS, 10_000):\n",
    "    if number_of_points == 0:\n",
    "        number_of_points = 1\n",
    "\n",
    "    print(f\"Creating collection with {number_of_points} points.\")\n",
    "    \n",
    "    points = create_points(number_of_points)\n",
    "    create_collection(points)\n",
    "\n",
    "    print(\"Waiting for collection status to be green.\")\n",
    "    while CLIENT.get_collection(COLLECTION_NAME).status.value == 'yellow':\n",
    "        time.sleep(30)\n",
    "        get_collection_info()\n",
    "\n",
    "    [query_vector] = create_vectors(n=1)\n",
    "    duration_wo_filter = search_collection(query_vector, n=100)\n",
    "    duration_top_level_filter = search_collection(query_vector, filter=get_top_level_filter(), n=100)\n",
    "    duration_nested_filter = search_collection(query_vector, filter=get_nested_filter(), n=100)\n",
    "\n",
    "    results.append({\n",
    "        'number_of_points': number_of_points,\n",
    "        'duration_wo_filter': duration_wo_filter,\n",
    "        'duration_top_level_filter': duration_top_level_filter,\n",
    "        'duration_nested_filter': duration_nested_filter,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aded1f-bae1-46fb-844e-5f85f95d2b66",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40666c7d-17b0-4a73-9df3-bc97322939f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [r['number_of_points' for r in results]\n",
    "y_wo_filter = [r['duration_wo_filter' for r in results]\n",
    "y_top_level_filter = [r['duration_top_level_filter' for r in results]\n",
    "y_nested_filter = [r['duration_nested_filter' for r in results]\n",
    "\n",
    "plt.plot(x, y_wo_filter, label='w/o filter')\n",
    "plt.plot(x, y_top_level_filter, label='top level filter')\n",
    "plt.plot(x, y_nested_filter, label='nested filter')\n",
    "\n",
    "\n",
    "plt.xlabel(\"Number of Points\")\n",
    "plt.ylabel(\"Duration in Seconds\")\n",
    "plt.title('Duration Search')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc4985-31bc-46f6-8184-0243cc4ea451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
